id: wakanda_business_intelligence_engine
namespace: assemblehack25.wakanda

description: |
  üèÜ WAKANDA DATA AWARD SUBMISSION
  Multi-source business intelligence with AI-powered decisions

inputs:
  - id: data_source_url
    type: STRING
    required: true

  - id: recipient_email
    type: STRING
    required: false

  - id: decision_threshold
    type: INT
    required: false

tasks:
  # PHASE 1: DOWNLOAD DATA
  - id: download_data
    type: io.kestra.plugin.fs.http.Download
    uri: "{{ inputs.data_source_url }}"
    failOnError: true

  # PHASE 2: CLEAN DATA
  - id: clean_data
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install pandas --quiet
    outputFiles:
      - "data_summary.json"
    script: |
      import pandas as pd
      import json
      import os
      import sys

      try:
          # Verify the downloaded file exists and is readable
          download_path = "{{ outputs.download_data.uri }}"
          if not os.path.exists(download_path):
              raise FileNotFoundError(f"Downloaded file not found: {download_path}")
          
          # Check file size (prevent processing extremely large files)
          file_size = os.path.getsize(download_path)
          max_size = 100 * 1024 * 1024  # 100MB limit
          if file_size > max_size:
              size_mb = file_size / (1024 * 1024)
              max_mb = max_size / (1024 * 1024)
              raise ValueError(
                  f"File too large: {size_mb:.1f}MB. "
                  f"Maximum allowed: {max_mb}MB. "
                  f"Please reduce file size or split into smaller files."
              )
          
          # Read CSV with error handling
          try:
              df = pd.read_csv(download_path, encoding='utf-8')
          except UnicodeDecodeError:
              # Try alternative encodings
              try:
                  df = pd.read_csv(download_path, encoding='latin-1')
              except (UnicodeDecodeError, pd.errors.EmptyDataError, pd.errors.ParserError):
                  try:
                      df = pd.read_csv(download_path, encoding='cp1252')
                  except Exception as e:
                      raise ValueError(f"Failed to read CSV with any encoding: {str(e)}")
          
          # Data validation and cleaning
          if df.empty:
              raise ValueError("Dataset is empty after loading")
          
          original_rows = len(df)
          df = df.dropna(how='all')
          cleaned_rows = len(df)
          
          # Limit columns for security (prevent excessive data exposure)
          max_columns = 50
          if len(df.columns) > max_columns:
              df = df.iloc[:, :max_columns]
              print(f"‚ö†Ô∏è  Limited to first {max_columns} columns for security")
          
          # Sanitize column names (prevent injection attacks and empty strings)
          sanitized_columns = []
          for i, col in enumerate(df.columns):
              # Convert to string and sanitize
              clean_col = str(col).replace('<', '').replace('>', '').replace('"', '').replace("'", '').strip()[:100]
              
              # Ensure column name is not empty
              if not clean_col or clean_col.isspace():
                  clean_col = f"column_{i}"
              
              # Ensure uniqueness
              original_col = clean_col
              counter = 1
              while clean_col in sanitized_columns:
                  clean_col = f"{original_col}_{counter}"
                  counter += 1
              
              sanitized_columns.append(clean_col)
          
          df.columns = sanitized_columns
          
          # Create safe summary with size limits
          sample_size = min(5, len(df))
          summary = {
              "total_rows": cleaned_rows,
              "original_rows": original_rows,
              "columns": list(df.columns)[:20],  # Limit column list
              "sample_data": df.head(sample_size).fillna('').astype(str).to_dict("records")
          }
          
          # Sanitize sample data to prevent injection
          for record in summary["sample_data"]:
              for key, value in record.items():
                  if isinstance(value, str) and len(value) > 200:
                      record[key] = value[:200] + "..."
          
          with open("data_summary.json", "w") as f:
              json.dump(summary, f, indent=2, default=str)
          
          print(f"‚úÖ Processed {cleaned_rows} rows ({original_rows - cleaned_rows} empty rows removed)")
          
      except Exception as e:
          error_summary = {
              "error": True,
              "message": str(e)[:500],  # Limit error message length
              "total_rows": 0,
              "columns": [],
              "sample_data": []
          }
          
          with open("data_summary.json", "w") as f:
              json.dump(error_summary, f, indent=2)
          
          print(f"‚ùå Error processing data: {str(e)}")
          sys.exit(1)

  # PHASE 3: AI ANALYSIS
  - id: ai_analysis
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install openai --quiet
    env:
      PERPLEXITY_API_KEY: "{{ kv('PERPLEXITY_API_KEY', 'system') }}"
    inputFiles:
      data_summary.json: "{{ outputs.clean_data.outputFiles['data_summary.json'] }}"
    outputFiles:
      - "ai_summary.txt"
    script: |
      from openai import OpenAI
      import json
      import os
      import time
      import sys

      def safe_api_call(client, messages, max_retries=3):
          """Make API call with retry logic and error handling"""
          for attempt in range(max_retries):
              try:
                  response = client.chat.completions.create(
                      model="sonar",
                      messages=messages,
                      max_tokens=1000,  # Limit response size
                      temperature=0.7
                  )
                  return response
              except Exception as e:
                  print(f"API call attempt {attempt + 1} failed: {str(e)}")
                  if attempt < max_retries - 1:
                      time.sleep(2 ** attempt)  # Exponential backoff
                  else:
                      raise e

      try:
          # Validate API key
          api_key = os.getenv("PERPLEXITY_API_KEY")
          if not api_key or len(api_key) < 10:
              raise ValueError("Invalid or missing PERPLEXITY_API_KEY")

          client = OpenAI(
              api_key=api_key,
              base_url="https://api.perplexity.ai"
          )

          with open("data_summary.json") as f:
              data = json.load(f)

          # Check for data processing errors
          if data.get("error"):
              raise ValueError(f"Data processing failed: {data.get('message', 'Unknown error')}")

          # Sanitize data for AI prompt (prevent prompt injection)
          safe_data = {
              "total_rows": data.get("total_rows", 0),
              "columns": [str(col)[:50] for col in data.get("columns", [])[:10]],  # Limit and sanitize
              "sample_size": len(data.get("sample_data", []))
          }

          # Create safe prompt template (no direct user data injection)
          system_prompt = """You are a professional business analyst. Analyze the provided dataset summary and provide structured insights. 
          Keep responses professional, factual, and under 500 words total. 
          Do not include any user-provided text verbatim in your response."""

          user_prompt = f"""Analyze this dataset summary:
          - Rows: {safe_data['total_rows']}
          - Columns: {len(safe_data['columns'])}
          - Column types: {', '.join(safe_data['columns'][:5])}

          Provide:
          ## SUMMARY
          2-3 sentences about the dataset

          ## INSIGHTS  
          3 bullet points about patterns or trends

          ## RECOMMENDATIONS
          3 actionable business recommendations"""

          messages = [
              {"role": "system", "content": system_prompt},
              {"role": "user", "content": user_prompt}
          ]

          response = safe_api_call(client, messages)
          summary = response.choices[0].message.content

          # Validate and sanitize AI response for meaningful content
          if not summary or len(summary.strip()) < 10:
              raise ValueError("AI response is empty or too short")
          
          # Check for meaningful content (not just repeated characters or whitespace)
          cleaned_summary = summary.strip()
          if len(set(cleaned_summary.replace(' ', '').replace('\n', ''))) < 5:
              raise ValueError("AI response lacks meaningful content")
          
          # Ensure response contains expected sections
          required_sections = ['SUMMARY', 'INSIGHTS', 'RECOMMENDATIONS']
          missing_sections = [section for section in required_sections if section not in summary.upper()]
          if len(missing_sections) > 1:  # Allow some flexibility
              print(f"‚ö†Ô∏è  AI response missing sections: {missing_sections}")
          
          summary = cleaned_summary

          # Limit response length for security
          if len(summary) > 2000:
              summary = summary[:2000] + "\n\n[Response truncated for security]"

          with open("ai_summary.txt", "w") as f:
              f.write(summary)

          print("=" * 60)
          print("AI ANALYSIS")
          print("=" * 60)
          print(summary)

      except Exception as e:
          error_message = f"AI Analysis failed: {str(e)[:200]}"
          
          with open("ai_summary.txt", "w") as f:
              f.write(f"Error: {error_message}")
          
          print(f"‚ùå {error_message}")
          sys.exit(1)

  # PHASE 4: AI DECISIONS
  - id: ai_decisions
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install openai --quiet
    env:
      PERPLEXITY_API_KEY: "{{ kv('PERPLEXITY_API_KEY', 'system') }}"
    inputFiles:
      ai_summary.txt: "{{ outputs.ai_analysis.outputFiles['ai_summary.txt'] }}"
    outputFiles:
      - "decisions.json"
    script: |
      from openai import OpenAI
      import json
      import os
      import re
      import time
      import sys

      def safe_api_call(client, messages, max_retries=3):
          """Make API call with retry logic and error handling"""
          for attempt in range(max_retries):
              try:
                  response = client.chat.completions.create(
                      model="sonar",
                      messages=messages,
                      max_tokens=500,  # Limit response size
                      temperature=0.3  # Lower temperature for more consistent JSON
                  )
                  return response
              except Exception as e:
                  print(f"API call attempt {attempt + 1} failed: {str(e)}")
                  if attempt < max_retries - 1:
                      time.sleep(2 ** attempt)
                  else:
                      raise e

      def extract_json_safely(text):
          """Safely extract JSON from AI response with robust error handling"""
          try:
              # Remove markdown code blocks
              text = re.sub(r'```json\s*', '', text)
              text = re.sub(r'```\s*', '', text)
              
              # Find JSON-like content
              json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text)
              if json_match:
                  json_str = json_match.group()
                  return json.loads(json_str)
              
              # If no JSON found, try parsing the entire text
              return json.loads(text.strip())
              
          except json.JSONDecodeError as e:
              print(f"JSON parsing failed: {e}")
              # Return safe default values
              return {
                  "impact_score": 50,
                  "confidence": 60,
                  "actions": ["Review data analysis", "Consult domain experts", "Gather additional data"],
                  "urgent": False
              }

      try:
          # Validate API key
          api_key = os.getenv("PERPLEXITY_API_KEY")
          if not api_key or len(api_key) < 10:
              raise ValueError("Invalid or missing PERPLEXITY_API_KEY")

          client = OpenAI(
              api_key=api_key,
              base_url="https://api.perplexity.ai"
          )

          with open("ai_summary.txt") as f:
              analysis = f.read()

          # Check if analysis contains error
          if analysis.startswith("Error:"):
              raise ValueError("Previous analysis step failed")

          # Sanitize analysis text (prevent prompt injection)
          analysis_safe = analysis[:800]  # Limit length
          analysis_safe = re.sub(r'[<>"\']', '', analysis_safe)  # Remove potentially dangerous chars

          # Create structured prompt that prevents injection
          system_prompt = """You are a business decision engine. Based on the analysis provided, return ONLY a valid JSON object with these exact fields:
          {
            "impact_score": <number 1-100>,
            "confidence": <number 1-100>, 
            "actions": [<array of 1-3 short action strings>],
            "urgent": <boolean>
          }
          
          Do not include any explanatory text, only the JSON object."""

          user_prompt = f"Analysis summary (first 800 chars): {analysis_safe}"

          messages = [
              {"role": "system", "content": system_prompt},
              {"role": "user", "content": user_prompt}
          ]

          response = safe_api_call(client, messages)
          response_text = response.choices[0].message.content.strip()

          # Safely extract JSON with robust error handling
          decisions = extract_json_safely(response_text)

          # Validate decision structure and values
          decisions["impact_score"] = max(1, min(100, int(decisions.get("impact_score", 50))))
          decisions["confidence"] = max(1, min(100, int(decisions.get("confidence", 60))))
          decisions["urgent"] = bool(decisions.get("urgent", False))
          
          # Sanitize and limit actions
          actions = decisions.get("actions", [])
          if not isinstance(actions, list):
              actions = ["Review analysis results"]
          
          # Limit to 3 actions and sanitize content
          safe_actions = []
          for action in actions[:3]:
              if isinstance(action, str) and len(action.strip()) > 0:
                  safe_action = re.sub(r'[<>"\']', '', str(action)[:100])
                  safe_actions.append(safe_action)
          
          if not safe_actions:
              safe_actions = ["Review data analysis", "Consult stakeholders"]
          
          decisions["actions"] = safe_actions

          with open("decisions.json", "w") as f:
              json.dump(decisions, f, indent=2)

          print("=" * 60)
          print("DECISIONS")
          print("=" * 60)
          print(json.dumps(decisions, indent=2))

      except Exception as e:
          # Create safe fallback decisions
          fallback_decisions = {
              "impact_score": 50,
              "confidence": 60,
              "actions": ["Manual review required", "Consult domain experts"],
              "urgent": False,
              "error": str(e)[:200]
          }
          
          with open("decisions.json", "w") as f:
              json.dump(fallback_decisions, f, indent=2)
          
          print(f"‚ùå Decision generation failed: {str(e)[:200]}")
          print("Using fallback decisions for safety")
          print(json.dumps(fallback_decisions, indent=2))

  # PHASE 5: CHECK & TRIGGER
  - id: check_and_trigger
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-slim
    inputFiles:
      decisions.json: "{{ outputs.ai_decisions.outputFiles['decisions.json'] }}"
    outputFiles:
      - "trigger.json"
    script: |
      import json

      with open("decisions.json") as f:
          decisions = json.load(f)

      impact = decisions.get("impact_score", 0)
      threshold = {{ inputs.decision_threshold }}
      trigger = impact >= threshold

      status = {
          "trigger": trigger,
          "impact": impact,
          "threshold": threshold
      }

      with open("trigger.json", "w") as f:
          json.dump(status, f)

      print("=" * 60)
      print("THRESHOLD CHECK")
      print("=" * 60)
      print(f"Impact: {impact}/100")
      print(f"Threshold: {threshold}/100")
      print(f"Triggered: {trigger}")
      print()

      if trigger:
          print("üöÄ AUTOMATED ACTIONS:")
          print("  ‚Ä¢ Marketing budget +20%")
          print("  ‚Ä¢ Hiring pipeline")
          print("  ‚Ä¢ Infrastructure optimization")

  # PHASE 6: FINAL REPORT
  - id: final_report
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-slim
    inputFiles:
      data_summary.json: "{{ outputs.clean_data.outputFiles['data_summary.json'] }}"
      ai_summary.txt: "{{ outputs.ai_analysis.outputFiles['ai_summary.txt'] }}"
      decisions.json: "{{ outputs.ai_decisions.outputFiles['decisions.json'] }}"
      trigger.json: "{{ outputs.check_and_trigger.outputFiles['trigger.json'] }}"
    script: |
      import json

      with open("data_summary.json") as f:
          data = json.load(f)
      
      with open("ai_summary.txt") as f:
          summary = f.read()
      
      with open("decisions.json") as f:
          decisions = json.load(f)
      
      with open("trigger.json") as f:
          status = json.load(f)

      print()
      print("=" * 75)
      print("üèÜ WAKANDA BUSINESS INTELLIGENCE REPORT")
      print("=" * 75)
      print()
      print(f"üìä DATA: {data['total_rows']} rows")
      print(f"ü§ñ AI: Perplexity Sonar")
      print()
      print("‚îÄ" * 75)
      print("AI ANALYSIS:")
      print("‚îÄ" * 75)
      print(summary)
      print()
      print("‚îÄ" * 75)
      print("AI DECISIONS:")
      print("‚îÄ" * 75)
      print(json.dumps(decisions, indent=2))
      print()
      print("‚îÄ" * 75)
      print("SUMMARY:")
      print("‚îÄ" * 75)
      print(f"  Impact: {status['impact']}/100")
      print(f"  Confidence: {decisions.get('confidence', 0)}%")
      print(f"  Automation: {'TRIGGERED' if status['trigger'] else 'SKIPPED'}")
      print()
      if status['trigger']:
          print("ACTIONS:")
          for i, action in enumerate(decisions.get('actions', [])[:3], 1):
              print(f"  {i}. {action}")
      print()
      print("=" * 75)

  # PHASE 7: STORE IN SUPABASE
  - id: store_in_supabase
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install supabase --quiet
    env:
      SUPABASE_URL: "{{ kv('SUPABASE_URL', 'system') }}"
      SUPABASE_KEY: "{{ kv('SUPABASE_KEY', 'system') }}"
    inputFiles:
      data_summary.json: "{{ outputs.clean_data.outputFiles['data_summary.json'] }}"
      ai_summary.txt: "{{ outputs.ai_analysis.outputFiles['ai_summary.txt'] }}"
      decisions.json: "{{ outputs.ai_decisions.outputFiles['decisions.json'] }}"
      trigger.json: "{{ outputs.check_and_trigger.outputFiles['trigger.json'] }}"
    script: |
      from supabase import create_client
      import json
      import os
      import time
      import sys
      from datetime import datetime, timezone

      def safe_supabase_call(table_func, max_retries=3):
          """Execute Supabase operation with retry logic"""
          for attempt in range(max_retries):
              try:
                  result = table_func()
                  return result
              except Exception as e:
                  print(f"Supabase attempt {attempt + 1} failed: {str(e)}")
                  if attempt < max_retries - 1:
                      time.sleep(2 ** attempt)
                  else:
                      raise e

      try:
          # Validate environment variables
          supabase_url = os.getenv("SUPABASE_URL")
          supabase_key = os.getenv("SUPABASE_KEY")
          
          if not supabase_url or not supabase_key:
              raise ValueError("Missing SUPABASE_URL or SUPABASE_KEY")

          supabase = create_client(supabase_url, supabase_key)

          # Load and validate all input files
          with open("data_summary.json") as f:
              data = json.load(f)
          
          with open("ai_summary.txt") as f:
              summary = f.read()
          
          with open("decisions.json") as f:
              decisions = json.load(f)
          
          with open("trigger.json") as f:
              status = json.load(f)

          # Calculate actual execution duration
          start_time = "{{ execution.startDate }}"
          current_time = datetime.now(timezone.utc).isoformat()
          
          # Parse start time and calculate duration
          try:
              start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
              current_dt = datetime.now(timezone.utc)
              duration_seconds = int((current_dt - start_dt).total_seconds())
              
              # Format duration as human readable
              if duration_seconds < 60:
                  duration = f"{duration_seconds}s"
              elif duration_seconds < 3600:
                  minutes = duration_seconds // 60
                  seconds = duration_seconds % 60
                  duration = f"{minutes}m {seconds}s"
              else:
                  hours = duration_seconds // 3600
                  minutes = (duration_seconds % 3600) // 60
                  duration = f"{hours}h {minutes}m"
          except (ValueError, TypeError, AttributeError) as e:
              print(f"Duration calculation failed: {e}")
              duration = "unknown"

          execution_id = "{{ execution.id }}"
          
          # Safely extract dataset name
          data_source_url = "{{ inputs.data_source_url }}"
          try:
              dataset_name = data_source_url.split("/")[-1][:100]  # Limit length
              if not dataset_name or dataset_name == "":
                  dataset_name = "unknown_dataset"
          except (AttributeError, IndexError, TypeError) as e:
              print(f"Dataset name extraction failed: {e}")
              dataset_name = "unknown_dataset"
          
          # Sanitize and validate data before insertion
          execution_data = {
              "id": execution_id[:50],  # Limit ID length
              "status": "success",
              "start_time": start_time,
              "duration": duration,
              "dataset_name": dataset_name,
              "dataset_rows": max(0, int(data.get("total_rows", 0))),
              "impact_score": max(0, min(100, int(status.get("impact", 0)))),
              "confidence": max(0, min(100, int(decisions.get("confidence", 0))))
          }

          # Insert execution record with retry
          safe_supabase_call(lambda: supabase.table("executions").insert(execution_data).execute())

          # Insert AI insights with length limits
          insights_data = {
              "execution_id": execution_id[:50],
              "summary": summary[:1000],  # Limit summary length
              "insights": decisions.get("actions", [])[:5],  # Limit array size
              "recommendations": decisions.get("actions", [])[:5]
          }
          
          safe_supabase_call(lambda: supabase.table("ai_insights").insert(insights_data).execute())

          # Insert decisions
          decisions_data = {
              "execution_id": execution_id[:50],
              "impact_score": max(0, min(100, int(status.get("impact", 0)))),
              "confidence": max(0, min(100, int(decisions.get("confidence", 0)))),
              "threshold": max(0, min(100, int(status.get("threshold", 70)))),
              "urgent": bool(decisions.get("urgent", False)),
              "actions": decisions.get("actions", [])[:5]  # Limit array size
          }
          
          safe_supabase_call(lambda: supabase.table("decisions").insert(decisions_data).execute())

          print("‚úÖ Stored in Supabase successfully!")
          print(f"   Execution ID: {execution_id}")
          print(f"   Duration: {duration}")
          print(f"   Impact Score: {execution_data['impact_score']}")

      except Exception as e:
          error_message = f"Supabase storage failed: {str(e)[:300]}"
          print(f"‚ùå {error_message}")
          
          # Don't fail the entire workflow for storage issues
          print("‚ö†Ô∏è  Continuing workflow despite storage failure")
          # sys.exit(1)  # Commented out to prevent workflow failure

triggers:
  - id: daily_run
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 9 * * MON-FRI"